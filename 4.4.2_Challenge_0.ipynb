{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from scipy import sparse\n",
    "from sklearn.svm import SVC\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')  Carroll\n",
       "4  ((, when, she, thought, it, over, afterwards, ...  Carroll"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hedge</th>\n",
       "      <th>rock</th>\n",
       "      <th>bind</th>\n",
       "      <th>desert</th>\n",
       "      <th>humoured</th>\n",
       "      <th>beau</th>\n",
       "      <th>savage</th>\n",
       "      <th>chief</th>\n",
       "      <th>pepper</th>\n",
       "      <th>ahem</th>\n",
       "      <th>...</th>\n",
       "      <th>lively</th>\n",
       "      <th>power</th>\n",
       "      <th>rapid</th>\n",
       "      <th>elliots</th>\n",
       "      <th>despair</th>\n",
       "      <th>sore</th>\n",
       "      <th>lazily</th>\n",
       "      <th>june</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  hedge rock bind desert humoured beau savage chief pepper ahem     ...      \\\n",
       "0     0    0    0      0        0    0      0     0      0    0     ...       \n",
       "1     0    0    0      0        0    0      0     0      0    0     ...       \n",
       "2     0    0    0      0        0    0      0     0      0    0     ...       \n",
       "3     0    0    0      0        0    0      0     0      0    0     ...       \n",
       "4     1    0    0      0        0    0      0     0      0    0     ...       \n",
       "\n",
       "  lively power rapid elliots despair sore lazily june  \\\n",
       "0      0     0     0       0       0    0      0    0   \n",
       "1      0     0     0       0       0    0      0    0   \n",
       "2      0     0     0       0       0    0      0    0   \n",
       "3      0     0     0       0       0    0      0    0   \n",
       "4      0     0     0       0       0    0      0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')     Carroll  \n",
       "4  ((, when, she, thought, it, over, afterwards, ...     Carroll  \n",
       "\n",
       "[5 rows x 3025 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.990088105727\n",
      "\n",
      "Test set score: 0.929515418502\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\n",
    "\n",
    "\n",
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2724, 3023) (2724,)\n",
      "Training set score: 0.963656387665\n",
      "\n",
      "Test set score: 0.935572687225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0642           27.14s\n",
      "         2           1.0133           27.09s\n",
      "         3           0.9749           26.80s\n",
      "         4           0.9402           26.87s\n",
      "         5           0.9123           27.14s\n",
      "         6           0.8877           27.92s\n",
      "         7           0.8673           27.28s\n",
      "         8           0.8490           26.73s\n",
      "         9           0.8328           26.32s\n",
      "        10           0.8186           26.55s\n",
      "        20           0.7222           24.70s\n",
      "        30           0.6672           21.62s\n",
      "        40           0.6251           18.44s\n",
      "        50           0.5947           15.41s\n",
      "        60           0.5698           12.27s\n",
      "        70           0.5458            9.18s\n",
      "        80           0.5279            6.09s\n",
      "        90           0.5093            3.03s\n",
      "       100           0.4936            0.00s\n",
      "Training set score: 0.92107195301\n",
      "\n",
      "Test set score: 0.90859030837\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier(verbose=True)\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Looks like logistic regression is the winner, but there's room for improvement.\n",
    "\n",
    "# Same model, new inputs\n",
    "\n",
    "What if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n",
    "\n",
    "First, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "\n",
    "# Emma is quite long, let's cut it down to the same length as Alice.\n",
    "emma_sents = emma_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.703466666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1158</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>535</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     1158       21\n",
       "Carroll     535      161"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n",
    "\n",
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 97%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hedge</th>\n",
       "      <th>rock</th>\n",
       "      <th>bind</th>\n",
       "      <th>desert</th>\n",
       "      <th>humoured</th>\n",
       "      <th>beau</th>\n",
       "      <th>savage</th>\n",
       "      <th>chief</th>\n",
       "      <th>pepper</th>\n",
       "      <th>ahem</th>\n",
       "      <th>...</th>\n",
       "      <th>truly</th>\n",
       "      <th>desk</th>\n",
       "      <th>lively</th>\n",
       "      <th>power</th>\n",
       "      <th>rapid</th>\n",
       "      <th>elliots</th>\n",
       "      <th>despair</th>\n",
       "      <th>sore</th>\n",
       "      <th>lazily</th>\n",
       "      <th>june</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tot</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_part</th>\n",
       "      <td>0.222093</td>\n",
       "      <td>0.185077</td>\n",
       "      <td>0.185077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740308</td>\n",
       "      <td>0.740308</td>\n",
       "      <td>0.185077</td>\n",
       "      <td>0.740308</td>\n",
       "      <td>0.740308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_part</th>\n",
       "      <td>0.181784</td>\n",
       "      <td>0.194769</td>\n",
       "      <td>0.194769</td>\n",
       "      <td>0.259692</td>\n",
       "      <td>0.259692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259692</td>\n",
       "      <td>0.259692</td>\n",
       "      <td>0.259692</td>\n",
       "      <td>0.259692</td>\n",
       "      <td>0.129846</td>\n",
       "      <td>0.259692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_part</th>\n",
       "      <td>0.181784</td>\n",
       "      <td>0.185077</td>\n",
       "      <td>0.185077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 3023 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              hedge      rock      bind    desert  humoured      beau  \\\n",
       "Carroll    3.000000  1.000000  1.000000  0.000000  0.000000  4.000000   \n",
       "Austen     7.000000  3.000000  3.000000  4.000000  3.000000  0.000000   \n",
       "tot       10.000000  4.000000  4.000000  4.000000  3.000000  4.000000   \n",
       "c_part     0.222093  0.185077  0.185077  0.000000  0.000000  0.740308   \n",
       "a_part     0.181784  0.194769  0.194769  0.259692  0.259692  0.000000   \n",
       "min_part   0.181784  0.185077  0.185077  0.000000  0.000000  0.000000   \n",
       "\n",
       "            savage     chief    pepper      ahem    ...         truly  \\\n",
       "Carroll   4.000000  1.000000  7.000000  1.000000    ...      0.000000   \n",
       "Austen    0.000000  3.000000  0.000000  0.000000    ...     12.000000   \n",
       "tot       4.000000  4.000000  7.000000  1.000000    ...     12.000000   \n",
       "c_part    0.740308  0.185077  0.740308  0.740308    ...      0.000000   \n",
       "a_part    0.000000  0.194769  0.000000  0.000000    ...      0.259692   \n",
       "min_part  0.000000  0.185077  0.000000  0.000000    ...      0.000000   \n",
       "\n",
       "              desk    lively      power     rapid   elliots   despair  \\\n",
       "Carroll   2.000000  0.000000   0.000000  0.000000  0.000000  1.000000   \n",
       "Austen    0.000000  6.000000  18.000000  4.000000  6.000000  1.000000   \n",
       "tot       2.000000  6.000000  18.000000  4.000000  6.000000  2.000000   \n",
       "c_part    0.740308  0.000000   0.000000  0.000000  0.000000  0.370154   \n",
       "a_part    0.000000  0.259692   0.259692  0.259692  0.259692  0.129846   \n",
       "min_part  0.000000  0.000000   0.000000  0.000000  0.000000  0.129846   \n",
       "\n",
       "              sore    lazily      june  \n",
       "Carroll   0.000000  1.000000  0.000000  \n",
       "Austen    3.000000  0.000000  3.000000  \n",
       "tot       3.000000  1.000000  3.000000  \n",
       "c_part    0.000000  0.740308  0.000000  \n",
       "a_part    0.259692  0.000000  0.259692  \n",
       "min_part  0.000000  0.000000  0.000000  \n",
       "\n",
       "[6 rows x 3023 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df = word_counts.drop(['text_sentence', 'text_source'], axis=1)[word_counts.text_source=='Carroll'].sum(axis=0)\n",
    "aus_df = word_counts.drop(['text_sentence', 'text_source'], axis=1)[word_counts.text_source=='Austen'].sum(axis=0)\n",
    "dfs = pd.DataFrame(columns=car_df.index, index=['Carroll', 'Austen'])\n",
    "dfs.loc['Carroll'] = car_df\n",
    "dfs.loc['Austen'] = aus_df\n",
    "dfs.loc['tot'] = dfs.iloc[:2].sum(axis=0)\n",
    "dfs.loc['c_part'] = dfs.loc['Carroll']/dfs.loc['tot']\n",
    "dfs.loc['a_part'] = dfs.loc['Austen']/dfs.loc['tot']\n",
    "class_imb = len(word_counts[word_counts.text_source=='Austen'])/len(word_counts)\n",
    "dfs.loc['c_part'] = dfs.loc['c_part'] * (class_imb)\n",
    "dfs.loc['a_part'] = dfs.loc['a_part'] * (1-class_imb)\n",
    "dfs.loc['min_part'] = dfs.iloc[-2:].min(axis=0)\n",
    "dfs.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4540, 2194)\n"
     ]
    }
   ],
   "source": [
    "filtered_words = np.where((dfs.loc['min_part'].values<.07), dfs.loc['min_part'].index, 'NaN')\n",
    "word_counts['length_'] = word_counts.text_sentence.apply(len)\n",
    "word_counts['has_!'] = word_counts.text_sentence.apply(lambda x: '!' in x.orth_)\n",
    "word_counts['has_num'] = word_counts.text_sentence.apply(lambda z: any([x.isnumeric() for x in z.orth_]))\n",
    "word_counts['has_alic'] = word_counts.text_sentence.apply(lambda x: ('Alice' in x.orth_))\n",
    "\n",
    "def get_mwl(sentence):\n",
    "    sentence = sentence.orth_\n",
    "    return pd.Series(sentence.split(' ')).apply(len).mean()\n",
    "    \n",
    "word_counts['mwl'] = word_counts.text_sentence.apply(get_mwl)\n",
    "word_counts['long_words'] = np.where(word_counts['mwl']>4.875, True, False)\n",
    "\n",
    "def get_longest_word(sentence):\n",
    "    sentence = sentence.orth_\n",
    "    return pd.Series(sentence.split(' ')).apply(len).max()\n",
    "    \n",
    "word_counts['longest_word'] = word_counts.text_sentence.apply(get_longest_word)\n",
    "\n",
    "def find_has_caps(sent):\n",
    "    for x in sent:\n",
    "        if len(x.orth_) > 1 and x.orth_.isupper():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "word_counts['has_all_caps'] = word_counts.text_sentence.apply(find_has_caps)\n",
    "\n",
    "filtered_words = list(set(filtered_words))\n",
    "filtered_words.remove('NaN')\n",
    "X = word_counts[filtered_words + ['has_all_caps']]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result: 96.7% W/ Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96718061674 {'alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "X = word_counts[filtered_words + ['has_all_caps']]\n",
    "mod = BernoulliNB()\n",
    "params = {\n",
    "    'alpha': [.5]\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(mod, params, cv=10, verbose=0, n_jobs=1)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=0.5, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BernoulliNB(alpha=.5)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Work and Notes Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "0.820704845815 {'alpha': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "X = word_counts[['longest_word', 'mwl', 'has_!', 'has_alic']]\n",
    "mod = BernoulliNB()\n",
    "params = {\n",
    "    'alpha': [.1, .3, .5, .7]\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(mod, params, cv=10, verbose=1, n_jobs=1)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25969162995594713"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yc = np.where(Y=='Carroll', 1, 0)\n",
    "Yc.sum()/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "\n",
      "mean model 1 score:  0.96718300303\n",
      "mean ensemble score:  0.964101723077\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "X = word_counts\n",
    "model_scores = []\n",
    "ensemble_scores = []\n",
    "got_wrong = []\n",
    "splt = StratifiedKFold(n_splits=10)\n",
    "for itrain, itest in splt.split(X, Y):\n",
    "    mod1 = BernoulliNB(alpha=.5)\n",
    "    mod1.fit(X[filtered_words + ['has_all_caps']].loc[itrain], Y.loc[itrain])\n",
    "    probs = mod1.predict_proba(X[filtered_words+ ['has_all_caps']].loc[itest])\n",
    "    preds1 = mod1.predict(X[filtered_words+ ['has_all_caps']].loc[itest])\n",
    "    high_conf = np.where(probs.max(axis=1)>.8, True, False)\n",
    "    low_conf = np.where(high_conf, False, True)\n",
    "    acc = np.where(preds1 == Y.loc[itest], 1, 0).sum()/len(Y.loc[itest])\n",
    "    #print('model 1 accuracy: ', acc)\n",
    "    model_scores.append(acc)\n",
    "    #print('accuracy at confidence threshold: ', np.where((high_conf) & (preds1==Y.loc[itest]), 1, 0).sum()/len(Y[high_conf]))\n",
    "    #print('proportion of predictions: ', (len(Y[high_conf])/len(Y.loc[itest])))\n",
    "\n",
    "    #mod2 = MultinomialNB(alpha=.1)\n",
    "\n",
    "    #mod2.fit(X[['longest_word', 'mwl', 'has_!', 'has_alic']].loc[itrain], Y.loc[itrain])\n",
    "    #preds2 = mod2.predict(X[['longest_word', 'mwl', 'has_!', 'has_alic']].loc[itest])\n",
    "    #master_preds = preds1\n",
    "    master_preds = np.where(low_conf & (X.loc[itest]['has_!']), 'Austen', preds1)\n",
    "    ens_score = np.where(master_preds==Y.loc[itest], 1, 0).sum()/len(Y.loc[itest])\n",
    "    #print('ensemble score: ', ens_score)\n",
    "    ensemble_scores.append(ens_score)\n",
    "\n",
    "    got_wrong += list(np.where((preds1 != Y.loc[itest]), itest, 0))\n",
    "    \n",
    "    \n",
    "print('-'*50, '\\n')\n",
    "print('mean model 1 score: ', pd.Series(model_scores).mean())\n",
    "print('mean ensemble score: ', pd.Series(ensemble_scores).mean())\n",
    "got_wrong=list(set(got_wrong))\n",
    "got_wrong.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mislabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18120805369127516"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mislabeled = word_counts.loc[got_wrong]\n",
    "len(mislabeled[(mislabeled.text_source=='Austen')])/len(mislabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[x.orth_.isupper() for x in mislabeled.iloc[11].text_sentence]\n",
    "\n",
    "def find_has_caps(sent):\n",
    "    for x in sent:\n",
    "        if len(x.orth_) > 1 and x.orth_.isupper():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "find_has_caps(mislabeled.iloc[12].text_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carroll'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mislabeled.iloc[12].text_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l1, score=0.8703296703296703, total=   1.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.8502202643171806, total=   0.9s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.8568281938325991, total=   0.9s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.8612334801762115, total=   1.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    6.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.8942731277533039, total=   1.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.8964757709251101, total=   1.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    9.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.9185022026431718, total=   1.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   11.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.8766519823788547, total=   1.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   13.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.8722466960352423, total=   1.1s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   14.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....... C=0.1, penalty=l1, score=0.869757174392936, total=   1.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.8857142857142857, total=   1.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.8744493392070485, total=   1.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.8700440528634361, total=   1.1s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.8766519823788547, total=   1.1s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.9185022026431718, total=   0.9s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.9273127753303965, total=   1.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.9317180616740088, total=   0.9s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.9162995594713657, total=   0.9s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.8942731277533039, total=   0.9s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.9028697571743929, total=   0.9s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.9032967032967033, total=   1.0s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.8722466960352423, total=   1.0s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.8810572687224669, total=   1.0s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.8898678414096917, total=   0.9s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.9251101321585903, total=   1.0s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ....... C=0.5, penalty=l1, score=0.947136563876652, total=   1.0s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.9449339207048458, total=   1.0s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.9405286343612335, total=   1.0s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.9185022026431718, total=   1.0s\n",
      "[CV] C=0.5, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l1, score=0.9205298013245033, total=   1.0s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.9230769230769231, total=   1.1s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.8964757709251101, total=   0.9s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.8920704845814978, total=   1.0s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.9008810572687225, total=   0.9s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.9273127753303965, total=   1.0s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.9559471365638766, total=   0.9s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.9449339207048458, total=   1.0s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ....... C=0.5, penalty=l2, score=0.933920704845815, total=   1.0s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.9140969162995595, total=   1.0s\n",
      "[CV] C=0.5, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.5, penalty=l2, score=0.9205298013245033, total=   0.9s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.9274725274725275, total=   1.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.8766519823788547, total=   1.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.8920704845814978, total=   1.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.8920704845814978, total=   1.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.9273127753303965, total=   1.1s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.9515418502202643, total=   1.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ......... C=1, penalty=l1, score=0.947136563876652, total=   1.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ......... C=1, penalty=l1, score=0.947136563876652, total=   1.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.9185022026431718, total=   1.1s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.9293598233995585, total=   1.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.9274725274725275, total=   1.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.9052863436123348, total=   1.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.8986784140969163, total=   1.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.9096916299559471, total=   1.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.9317180616740088, total=   1.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.9559471365638766, total=   1.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.9449339207048458, total=   1.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ......... C=1, penalty=l2, score=0.933920704845815, total=   1.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.9162995594713657, total=   1.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.9315673289183223, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.9296703296703297, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.8766519823788547, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.8942731277533039, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.8986784140969163, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.9317180616740088, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.9493392070484582, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.9317180616740088, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ C=5, penalty=l1, score=0.9361233480176211, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.9162995594713657, total=   1.0s\n",
      "[CV] C=5, penalty=l1 .................................................\n",
      "[CV] ........ C=5, penalty=l1, score=0.9403973509933775, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ........ C=5, penalty=l2, score=0.9428571428571428, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ........ C=5, penalty=l2, score=0.9140969162995595, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ........ C=5, penalty=l2, score=0.8986784140969163, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ........ C=5, penalty=l2, score=0.9118942731277533, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ........ C=5, penalty=l2, score=0.9251101321585903, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ........ C=5, penalty=l2, score=0.9559471365638766, total=   0.9s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ........ C=5, penalty=l2, score=0.9449339207048458, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ......... C=5, penalty=l2, score=0.933920704845815, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ......... C=5, penalty=l2, score=0.920704845814978, total=   1.0s\n",
      "[CV] C=5, penalty=l2 .................................................\n",
      "[CV] ........ C=5, penalty=l2, score=0.9359823399558499, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.9274725274725275, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.8788546255506607, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.8898678414096917, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.8898678414096917, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ........ C=10, penalty=l1, score=0.933920704845815, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ........ C=10, penalty=l1, score=0.947136563876652, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.9317180616740088, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.9273127753303965, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.9162995594713657, total=   1.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.9381898454746137, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ........ C=10, penalty=l2, score=0.945054945054945, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.9074889867841409, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.8986784140969163, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.9162995594713657, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.9295154185022027, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.9559471365638766, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.9493392070484582, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ........ C=10, penalty=l2, score=0.933920704845815, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.9229074889867841, total=   1.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.9359823399558499, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929515418502 {'C': 10, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "linreg = LogisticRegression()\n",
    "X = word_counts.drop(['text_source', 'text_sentence'], axis=1)\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [.1, .5, 1, 5, 10]\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(linreg, params, cv=10, verbose=10, n_jobs=1)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alliedtoasters/anaconda3/envs/dspy3/lib/python3.6/site-packages/pandas/core/internals.py:258: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 4540 but corresponding boolean dimension is 909\n",
      "  return self.values[slicer]\n",
      "/home/alliedtoasters/anaconda3/envs/dspy3/lib/python3.6/site-packages/pandas/core/indexes/base.py:1700: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 4540 but corresponding boolean dimension is 909\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy at confidence threshold:  0.965440356745\n",
      "proportion of predictions:  0.9867986798679867\n",
      "ensemble score:  0.95599559956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alliedtoasters/anaconda3/envs/dspy3/lib/python3.6/site-packages/pandas/core/internals.py:258: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 4540 but corresponding boolean dimension is 908\n",
      "  return self.values[slicer]\n",
      "/home/alliedtoasters/anaconda3/envs/dspy3/lib/python3.6/site-packages/pandas/core/indexes/base.py:1700: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 4540 but corresponding boolean dimension is 908\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy at confidence threshold:  0.954796030871\n",
      "proportion of predictions:  0.998898678414097\n",
      "ensemble score:  0.954845814978\n",
      "accuracy at confidence threshold:  0.973539140022\n",
      "proportion of predictions:  0.998898678414097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-1608ddf960be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmod2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mpreds2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmaster_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dspy3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1034\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dspy3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dspy3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 788\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dspy3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dspy3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "splt = StratifiedKFold(n_splits=5)\n",
    "for itrain, itest in splt.split(X, Y):\n",
    "    mod1 = BernoulliNB(alpha=.5)\n",
    "    mod1.fit(X.loc[itrain], Y.loc[itrain])\n",
    "    probs = mod1.predict_proba(X.loc[itest])\n",
    "    preds1 = mod1.predict(X.loc[itest])\n",
    "    high_conf = np.where(probs.max(axis=1)>.55, True, False)\n",
    "    low_conf = np.where(high_conf, False, True)\n",
    "    print('accuracy at confidence threshold: ', np.where((high_conf) & (preds1==Y.loc[itest]), 1, 0).sum()/len(Y[high_conf]))\n",
    "    print('proportion of predictions: ', (len(Y[high_conf])/len(Y.loc[itest])))\n",
    "\n",
    "    mod2 = ensemble.GradientBoostingClassifier(\n",
    "        verbose=False, \n",
    "        n_estimators=6000,\n",
    "        learning_rate=.05,\n",
    "        subsample=.18,\n",
    "        max_features=15\n",
    "    )\n",
    "    mod2.fit(X.loc[itrain], Y.loc[itrain])\n",
    "    preds2 = mod2.predict(X.loc[itest])\n",
    "    master_preds = np.where(high_conf, preds1, preds2)\n",
    "    ens_score = np.where(master_preds==Y.loc[itest], 1, 0).sum()/len(Y.loc[itest])\n",
    "    print('ensemble score: ', ens_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.asarray(X)\n",
    "X = sparse.coo_matrix(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.840153714521477"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'class_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-973551c19c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m params = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'class_weight'"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "clf = ensemble.GradientBoostingClassifier(verbose=True, class_weight='balanced')\n",
    "params = {\n",
    "    'n_estimators': [6000],\n",
    "    'learning_rate': [.05],\n",
    "    'subsample': [.18],\n",
    "    'max_features': [15],\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(clf, params, cv=5, verbose=10, n_jobs=1)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.25"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(word_counts[word_counts.text_source=='Carroll'].text_sentence.iloc[45].orth_.split(' ')).apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Few women could think more of their personal appearance than he did, nor could the valet of any new made lord be more delighted with the place he held in society."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[word_counts.text_source=='Austen'].text_sentence.iloc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.match(r'[A-Z][A-Z]+', 'AL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eldest', 'italian', 'merrily', 'middle', 'rap', 'connect', 'delicate',\n",
       "       'intend', 'point', 'expression',\n",
       "       ...\n",
       "       'kindness', 'ful', 'surprised', 'indisposition', 'essential', 'utility',\n",
       "       'cupboard', 'text_sentence', 'text_source', 'length_'],\n",
       "      dtype='object', length=3026)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sparse.coo_matrix(X)\n",
    "\n",
    "svc = SVC(class_weight='balanced')\n",
    "params = {\n",
    "    'C': [.1, 1, 10],\n",
    "    'gamma': [.1, 1]\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(svc, params, cv=10, verbose=2, n_jobs=2)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.asarray(X), \n",
    "                                                    np.asarray(Y),\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "Y_train = np.where(y_train=='Austen', 1, 0)\n",
    "Y_test = np.where(y_test=='Austen', 1, 0)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_imb = len(word_counts[word_counts.text_source=='Austen'])/len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 6, \n",
    "    'eta': .05,\n",
    "    'silent': 0, \n",
    "    'objective': \n",
    "    'binary:logistic', \n",
    "    'scale_pos_weight': (class_imb),  #Tell model about class imbalance\n",
    "    'nthread':4,\n",
    "    'eval_metric':'error'\n",
    "}\n",
    "\n",
    "plst = param.items()\n",
    "num_round = 500\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "bst = xgb.train(plst, dtrain, num_round, evallist, verbose_eval=True) #We can watch outputs to see model slowly optimize.\n",
    "#Breaks 90% accuracy at around 410 iterations, with vanishing returns around 500.\n",
    "\n",
    "Y_ = np.where(bst.predict(dtrain)>.5, 1, 0)\n",
    "tb = pd.crosstab(Y_, Y_train)\n",
    "print(tb)\n",
    "print((tb.iloc[0, 0]+tb.iloc[1, 1])/tb.sum().sum())\n",
    "\n",
    "Y_ = np.where(bst.predict(dtest)>.5, 1, 0)\n",
    "tb = pd.crosstab(Y_, Y_test)\n",
    "print(tb)\n",
    "print((tb.iloc[0, 0]+tb.iloc[1, 1])/tb.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_pos = []\n",
    "for sentence in word_counts['text_sentence']:\n",
    "    for token in sentence:\n",
    "        unique_pos.append(token.pos_)\n",
    "        \n",
    "unique_pos = list(set(unique_pos))\n",
    "\n",
    "def pos_features(sentences, unique_pos):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns = unique_pos, index=pd.Series(sentences).index)\n",
    "    df.loc[:, unique_pos] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        poss = [token.pos_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for pos in poss:\n",
    "            df.loc[i, pos] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "pos_counts = pos_features(word_counts.text_sentence, unique_pos)\n",
    "pos_counts = pos_counts.drop(['CCONJ'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_densities = []\n",
    "for pos in pos_counts.columns:\n",
    "    pos_counts['{}_density'.format(pos)] = pos_counts[pos]/word_counts['length_']\n",
    "    pos_densities.append('{}_density'.format(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:   17.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957929515419 {'alpha': 0.8}\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(pd.concat([word_counts[filtered_words], pos_counts[pos_densities]], axis=1))\n",
    "mod = MultinomialNB()\n",
    "params = {\n",
    "    'alpha': [.8, .9]\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(mod, params, cv=5, verbose=1, n_jobs=2)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_deps = []\n",
    "for sent in word_counts.text_sentence:\n",
    "    last_deps.append(sent[0].dep_)\n",
    "    \n",
    "last_deps = pd.Series(last_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] alpha=1 .........................................................\n",
      "[CV] .......................................... alpha=1, total=   1.0s\n",
      "[CV] alpha=1 .........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .......................................... alpha=1, total=   1.0s\n",
      "[CV] alpha=1 .........................................................\n",
      "[CV] .......................................... alpha=1, total=   0.9s\n",
      "[CV] alpha=1 .........................................................\n",
      "[CV] .......................................... alpha=1, total=   1.0s\n",
      "[CV] alpha=1 .........................................................\n",
      "[CV] .......................................... alpha=1, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.945814977974 {'alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "first_deps = []\n",
    "for sent in word_counts.text_sentence:\n",
    "    first_deps.append(sent[0].dep_)\n",
    "    \n",
    "first_deps = pd.Series(first_deps)\n",
    "X = pd.concat([pd.get_dummies(first_deps), word_counts[filtered_words]], axis=1)\n",
    "mod = BernoulliNB()\n",
    "params = {\n",
    "    'alpha': [1]\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(mod, params, cv=5, verbose=2)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just throw everything at a more robust model.\n",
    "X = np.asarray(pd.concat([word_counts[filtered_words], pd.get_dummies(first_deps), pos_counts[pos_densities]], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.131057\ttrain-error:0.123715\n",
      "[1]\teval-error:0.129956\ttrain-error:0.121512\n",
      "[2]\teval-error:0.132159\ttrain-error:0.129222\n",
      "[3]\teval-error:0.129405\ttrain-error:0.122247\n",
      "[4]\teval-error:0.127203\ttrain-error:0.117841\n",
      "[5]\teval-error:0.127203\ttrain-error:0.120411\n",
      "[6]\teval-error:0.128855\ttrain-error:0.118576\n",
      "[7]\teval-error:0.126101\ttrain-error:0.116006\n",
      "[8]\teval-error:0.125\ttrain-error:0.114537\n",
      "[9]\teval-error:0.124449\ttrain-error:0.113803\n",
      "[10]\teval-error:0.123899\ttrain-error:0.113436\n",
      "[11]\teval-error:0.122797\ttrain-error:0.111233\n",
      "[12]\teval-error:0.119493\ttrain-error:0.108664\n",
      "[13]\teval-error:0.118943\ttrain-error:0.108297\n",
      "[14]\teval-error:0.11674\ttrain-error:0.106094\n",
      "[15]\teval-error:0.114537\ttrain-error:0.102423\n",
      "[16]\teval-error:0.114537\ttrain-error:0.10022\n",
      "[17]\teval-error:0.112885\ttrain-error:0.098752\n",
      "[18]\teval-error:0.112885\ttrain-error:0.098018\n",
      "[19]\teval-error:0.111784\ttrain-error:0.096916\n",
      "[20]\teval-error:0.110683\ttrain-error:0.094714\n",
      "[21]\teval-error:0.109582\ttrain-error:0.093612\n",
      "[22]\teval-error:0.110132\ttrain-error:0.093612\n",
      "[23]\teval-error:0.105176\ttrain-error:0.091043\n",
      "[24]\teval-error:0.102423\ttrain-error:0.08884\n",
      "[25]\teval-error:0.10022\ttrain-error:0.087372\n",
      "[26]\teval-error:0.100771\ttrain-error:0.084802\n",
      "[27]\teval-error:0.10022\ttrain-error:0.084068\n",
      "[28]\teval-error:0.098018\ttrain-error:0.0837\n",
      "[29]\teval-error:0.098568\ttrain-error:0.082966\n",
      "[30]\teval-error:0.099119\ttrain-error:0.082232\n",
      "[31]\teval-error:0.095264\ttrain-error:0.082232\n",
      "[32]\teval-error:0.095264\ttrain-error:0.082232\n",
      "[33]\teval-error:0.093612\ttrain-error:0.081498\n",
      "[34]\teval-error:0.093062\ttrain-error:0.081498\n",
      "[35]\teval-error:0.093062\ttrain-error:0.081498\n",
      "[36]\teval-error:0.094714\ttrain-error:0.081131\n",
      "[37]\teval-error:0.093062\ttrain-error:0.080396\n",
      "[38]\teval-error:0.093062\ttrain-error:0.080029\n",
      "[39]\teval-error:0.094714\ttrain-error:0.079295\n",
      "[40]\teval-error:0.093612\ttrain-error:0.079295\n",
      "[41]\teval-error:0.093062\ttrain-error:0.079295\n",
      "[42]\teval-error:0.093062\ttrain-error:0.079295\n",
      "[43]\teval-error:0.092511\ttrain-error:0.07746\n",
      "[44]\teval-error:0.093612\ttrain-error:0.076725\n",
      "[45]\teval-error:0.09196\ttrain-error:0.077093\n",
      "[46]\teval-error:0.092511\ttrain-error:0.075991\n",
      "[47]\teval-error:0.092511\ttrain-error:0.075991\n",
      "[48]\teval-error:0.089758\ttrain-error:0.075624\n",
      "[49]\teval-error:0.090859\ttrain-error:0.075624\n",
      "[50]\teval-error:0.09141\ttrain-error:0.075257\n",
      "[51]\teval-error:0.09141\ttrain-error:0.075257\n",
      "[52]\teval-error:0.090859\ttrain-error:0.074523\n",
      "[53]\teval-error:0.090859\ttrain-error:0.074523\n",
      "[54]\teval-error:0.090859\ttrain-error:0.074523\n",
      "[55]\teval-error:0.090308\ttrain-error:0.073789\n",
      "[56]\teval-error:0.089207\ttrain-error:0.073789\n",
      "[57]\teval-error:0.088656\ttrain-error:0.073421\n",
      "[58]\teval-error:0.088656\ttrain-error:0.073054\n",
      "[59]\teval-error:0.088656\ttrain-error:0.073054\n",
      "[60]\teval-error:0.089207\ttrain-error:0.072687\n",
      "[61]\teval-error:0.089207\ttrain-error:0.072687\n",
      "[62]\teval-error:0.089207\ttrain-error:0.072687\n",
      "[63]\teval-error:0.089207\ttrain-error:0.073054\n",
      "[64]\teval-error:0.087555\ttrain-error:0.07232\n",
      "[65]\teval-error:0.088106\ttrain-error:0.071953\n",
      "[66]\teval-error:0.088106\ttrain-error:0.071953\n",
      "[67]\teval-error:0.088106\ttrain-error:0.071586\n",
      "[68]\teval-error:0.088106\ttrain-error:0.071586\n",
      "[69]\teval-error:0.088106\ttrain-error:0.071586\n",
      "[70]\teval-error:0.088656\ttrain-error:0.071219\n",
      "[71]\teval-error:0.088656\ttrain-error:0.071586\n",
      "[72]\teval-error:0.088656\ttrain-error:0.071219\n",
      "[73]\teval-error:0.088656\ttrain-error:0.071219\n",
      "[74]\teval-error:0.088106\ttrain-error:0.06975\n",
      "[75]\teval-error:0.087555\ttrain-error:0.06975\n",
      "[76]\teval-error:0.087555\ttrain-error:0.06975\n",
      "[77]\teval-error:0.087555\ttrain-error:0.069383\n",
      "[78]\teval-error:0.087555\ttrain-error:0.069016\n",
      "[79]\teval-error:0.087555\ttrain-error:0.069016\n",
      "[80]\teval-error:0.087555\ttrain-error:0.068282\n",
      "[81]\teval-error:0.088106\ttrain-error:0.068282\n",
      "[82]\teval-error:0.087555\ttrain-error:0.068282\n",
      "[83]\teval-error:0.087555\ttrain-error:0.068282\n",
      "[84]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[85]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[86]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[87]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[88]\teval-error:0.087004\ttrain-error:0.068282\n",
      "[89]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[90]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[91]\teval-error:0.087004\ttrain-error:0.067548\n",
      "[92]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[93]\teval-error:0.087004\ttrain-error:0.067548\n",
      "[94]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[95]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[96]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[97]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[98]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[99]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[100]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[101]\teval-error:0.087004\ttrain-error:0.067915\n",
      "[102]\teval-error:0.086454\ttrain-error:0.067181\n",
      "[103]\teval-error:0.085903\ttrain-error:0.067181\n",
      "[104]\teval-error:0.085903\ttrain-error:0.067181\n",
      "[105]\teval-error:0.085903\ttrain-error:0.065345\n",
      "[106]\teval-error:0.085903\ttrain-error:0.065345\n",
      "[107]\teval-error:0.085352\ttrain-error:0.065345\n",
      "[108]\teval-error:0.085352\ttrain-error:0.065712\n",
      "[109]\teval-error:0.085352\ttrain-error:0.065712\n",
      "[110]\teval-error:0.085352\ttrain-error:0.065712\n",
      "[111]\teval-error:0.085352\ttrain-error:0.065345\n",
      "[112]\teval-error:0.085352\ttrain-error:0.064978\n",
      "[113]\teval-error:0.085352\ttrain-error:0.064611\n",
      "[114]\teval-error:0.085352\ttrain-error:0.064611\n",
      "[115]\teval-error:0.085903\ttrain-error:0.064244\n",
      "[116]\teval-error:0.085903\ttrain-error:0.064244\n",
      "[117]\teval-error:0.085903\ttrain-error:0.064244\n",
      "[118]\teval-error:0.085903\ttrain-error:0.064244\n",
      "[119]\teval-error:0.085903\ttrain-error:0.065345\n",
      "[120]\teval-error:0.085903\ttrain-error:0.064244\n",
      "[121]\teval-error:0.085903\ttrain-error:0.064244\n",
      "[122]\teval-error:0.085903\ttrain-error:0.065345\n",
      "[123]\teval-error:0.085903\ttrain-error:0.065345\n",
      "[124]\teval-error:0.085903\ttrain-error:0.065345\n",
      "[125]\teval-error:0.086454\ttrain-error:0.065345\n",
      "[126]\teval-error:0.085903\ttrain-error:0.064611\n",
      "[127]\teval-error:0.08315\ttrain-error:0.064244\n",
      "[128]\teval-error:0.084251\ttrain-error:0.066079\n",
      "[129]\teval-error:0.084802\ttrain-error:0.064611\n",
      "[130]\teval-error:0.084802\ttrain-error:0.064978\n",
      "[131]\teval-error:0.084802\ttrain-error:0.064611\n",
      "[132]\teval-error:0.084251\ttrain-error:0.064244\n",
      "[133]\teval-error:0.0837\ttrain-error:0.065345\n",
      "[134]\teval-error:0.0837\ttrain-error:0.064978\n",
      "[135]\teval-error:0.0837\ttrain-error:0.064244\n",
      "[136]\teval-error:0.084251\ttrain-error:0.063877\n",
      "[137]\teval-error:0.085352\ttrain-error:0.06351\n",
      "[138]\teval-error:0.084802\ttrain-error:0.063142\n",
      "[139]\teval-error:0.085352\ttrain-error:0.063142\n",
      "[140]\teval-error:0.085352\ttrain-error:0.063142\n",
      "[141]\teval-error:0.085352\ttrain-error:0.062408\n",
      "[142]\teval-error:0.085352\ttrain-error:0.062775\n",
      "[143]\teval-error:0.085352\ttrain-error:0.062775\n",
      "[144]\teval-error:0.086454\ttrain-error:0.062408\n",
      "[145]\teval-error:0.082599\ttrain-error:0.06094\n",
      "[146]\teval-error:0.0837\ttrain-error:0.059838\n",
      "[147]\teval-error:0.0837\ttrain-error:0.059838\n",
      "[148]\teval-error:0.084251\ttrain-error:0.059471\n",
      "[149]\teval-error:0.084251\ttrain-error:0.059471\n",
      "[150]\teval-error:0.084251\ttrain-error:0.059838\n",
      "[151]\teval-error:0.084251\ttrain-error:0.059838\n",
      "[152]\teval-error:0.084251\ttrain-error:0.059838\n",
      "[153]\teval-error:0.084251\ttrain-error:0.059838\n",
      "[154]\teval-error:0.084251\ttrain-error:0.059838\n",
      "[155]\teval-error:0.0837\ttrain-error:0.059838\n",
      "[156]\teval-error:0.084251\ttrain-error:0.059838\n",
      "[157]\teval-error:0.085352\ttrain-error:0.059104\n",
      "[158]\teval-error:0.085352\ttrain-error:0.057636\n",
      "[159]\teval-error:0.084802\ttrain-error:0.056535\n",
      "[160]\teval-error:0.084251\ttrain-error:0.056535\n",
      "[161]\teval-error:0.086454\ttrain-error:0.056902\n",
      "[162]\teval-error:0.086454\ttrain-error:0.056535\n",
      "[163]\teval-error:0.086454\ttrain-error:0.056902\n",
      "[164]\teval-error:0.086454\ttrain-error:0.056167\n",
      "[165]\teval-error:0.085903\ttrain-error:0.056167\n",
      "[166]\teval-error:0.087004\ttrain-error:0.055066\n",
      "[167]\teval-error:0.087004\ttrain-error:0.055066\n",
      "[168]\teval-error:0.086454\ttrain-error:0.054332\n",
      "[169]\teval-error:0.086454\ttrain-error:0.054332\n",
      "[170]\teval-error:0.086454\ttrain-error:0.054699\n",
      "[171]\teval-error:0.087555\ttrain-error:0.053598\n",
      "[172]\teval-error:0.087555\ttrain-error:0.053598\n",
      "[173]\teval-error:0.089207\ttrain-error:0.053231\n",
      "[174]\teval-error:0.088656\ttrain-error:0.052129\n",
      "[175]\teval-error:0.089207\ttrain-error:0.052129\n",
      "[176]\teval-error:0.088656\ttrain-error:0.052129\n",
      "[177]\teval-error:0.088656\ttrain-error:0.052129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178]\teval-error:0.088656\ttrain-error:0.052129\n",
      "[179]\teval-error:0.088656\ttrain-error:0.051762\n",
      "[180]\teval-error:0.089207\ttrain-error:0.051028\n",
      "[181]\teval-error:0.089207\ttrain-error:0.051028\n",
      "[182]\teval-error:0.089207\ttrain-error:0.050661\n",
      "[183]\teval-error:0.088656\ttrain-error:0.051028\n",
      "[184]\teval-error:0.088656\ttrain-error:0.051028\n",
      "[185]\teval-error:0.088106\ttrain-error:0.050294\n",
      "[186]\teval-error:0.088656\ttrain-error:0.049927\n",
      "[187]\teval-error:0.088656\ttrain-error:0.049927\n",
      "[188]\teval-error:0.088656\ttrain-error:0.049559\n",
      "[189]\teval-error:0.089758\ttrain-error:0.049192\n",
      "[190]\teval-error:0.089758\ttrain-error:0.049192\n",
      "[191]\teval-error:0.089207\ttrain-error:0.048825\n",
      "[192]\teval-error:0.087555\ttrain-error:0.047357\n",
      "[193]\teval-error:0.087555\ttrain-error:0.047724\n",
      "[194]\teval-error:0.087555\ttrain-error:0.047357\n",
      "[195]\teval-error:0.087004\ttrain-error:0.04699\n",
      "[196]\teval-error:0.087004\ttrain-error:0.04699\n",
      "[197]\teval-error:0.087004\ttrain-error:0.047357\n",
      "[198]\teval-error:0.087004\ttrain-error:0.04699\n",
      "[199]\teval-error:0.090859\ttrain-error:0.046623\n",
      "[200]\teval-error:0.090859\ttrain-error:0.046623\n",
      "[201]\teval-error:0.09196\ttrain-error:0.045888\n",
      "[202]\teval-error:0.090859\ttrain-error:0.046256\n",
      "[203]\teval-error:0.090859\ttrain-error:0.046623\n",
      "[204]\teval-error:0.090859\ttrain-error:0.046623\n",
      "[205]\teval-error:0.09141\ttrain-error:0.045154\n",
      "[206]\teval-error:0.09196\ttrain-error:0.044787\n",
      "[207]\teval-error:0.092511\ttrain-error:0.04442\n",
      "[208]\teval-error:0.092511\ttrain-error:0.043686\n",
      "[209]\teval-error:0.092511\ttrain-error:0.043686\n",
      "[210]\teval-error:0.092511\ttrain-error:0.043319\n",
      "[211]\teval-error:0.092511\ttrain-error:0.043319\n",
      "[212]\teval-error:0.092511\ttrain-error:0.042952\n",
      "[213]\teval-error:0.092511\ttrain-error:0.042952\n",
      "[214]\teval-error:0.092511\ttrain-error:0.043319\n",
      "[215]\teval-error:0.092511\ttrain-error:0.043319\n",
      "[216]\teval-error:0.09196\ttrain-error:0.043686\n",
      "[217]\teval-error:0.09196\ttrain-error:0.043319\n",
      "[218]\teval-error:0.092511\ttrain-error:0.043319\n",
      "[219]\teval-error:0.092511\ttrain-error:0.043319\n",
      "[220]\teval-error:0.092511\ttrain-error:0.043319\n",
      "[221]\teval-error:0.093062\ttrain-error:0.043319\n",
      "[222]\teval-error:0.093062\ttrain-error:0.042952\n",
      "[223]\teval-error:0.093062\ttrain-error:0.042217\n",
      "[224]\teval-error:0.093612\ttrain-error:0.04185\n",
      "[225]\teval-error:0.093062\ttrain-error:0.04185\n",
      "[226]\teval-error:0.093062\ttrain-error:0.04185\n",
      "[227]\teval-error:0.093612\ttrain-error:0.04185\n",
      "[228]\teval-error:0.093612\ttrain-error:0.041483\n",
      "[229]\teval-error:0.093062\ttrain-error:0.040749\n",
      "[230]\teval-error:0.093062\ttrain-error:0.040749\n",
      "[231]\teval-error:0.093612\ttrain-error:0.040382\n",
      "[232]\teval-error:0.094163\ttrain-error:0.040382\n",
      "[233]\teval-error:0.094163\ttrain-error:0.040015\n",
      "[234]\teval-error:0.094714\ttrain-error:0.040015\n",
      "[235]\teval-error:0.093062\ttrain-error:0.03928\n",
      "[236]\teval-error:0.093062\ttrain-error:0.03928\n",
      "[237]\teval-error:0.094163\ttrain-error:0.03928\n",
      "[238]\teval-error:0.094163\ttrain-error:0.03928\n",
      "[239]\teval-error:0.094163\ttrain-error:0.039648\n",
      "[240]\teval-error:0.093612\ttrain-error:0.039648\n",
      "[241]\teval-error:0.093612\ttrain-error:0.039648\n",
      "[242]\teval-error:0.09196\ttrain-error:0.040015\n",
      "[243]\teval-error:0.092511\ttrain-error:0.039648\n",
      "[244]\teval-error:0.09196\ttrain-error:0.039648\n",
      "[245]\teval-error:0.093062\ttrain-error:0.039648\n",
      "[246]\teval-error:0.094163\ttrain-error:0.039648\n",
      "[247]\teval-error:0.094163\ttrain-error:0.038913\n",
      "[248]\teval-error:0.093612\ttrain-error:0.038913\n",
      "[249]\teval-error:0.093062\ttrain-error:0.038913\n",
      "[250]\teval-error:0.093062\ttrain-error:0.038913\n",
      "[251]\teval-error:0.093062\ttrain-error:0.038913\n",
      "[252]\teval-error:0.093062\ttrain-error:0.038179\n",
      "[253]\teval-error:0.094163\ttrain-error:0.038179\n",
      "[254]\teval-error:0.093612\ttrain-error:0.037445\n",
      "[255]\teval-error:0.093612\ttrain-error:0.037445\n",
      "[256]\teval-error:0.094163\ttrain-error:0.037812\n",
      "[257]\teval-error:0.094163\ttrain-error:0.037812\n",
      "[258]\teval-error:0.094163\ttrain-error:0.037445\n",
      "[259]\teval-error:0.094163\ttrain-error:0.037078\n",
      "[260]\teval-error:0.094163\ttrain-error:0.037078\n",
      "[261]\teval-error:0.093612\ttrain-error:0.037078\n",
      "[262]\teval-error:0.093612\ttrain-error:0.036711\n",
      "[263]\teval-error:0.093062\ttrain-error:0.036344\n",
      "[264]\teval-error:0.09196\ttrain-error:0.035977\n",
      "[265]\teval-error:0.09196\ttrain-error:0.035977\n",
      "[266]\teval-error:0.09196\ttrain-error:0.036344\n",
      "[267]\teval-error:0.093062\ttrain-error:0.035977\n",
      "[268]\teval-error:0.092511\ttrain-error:0.035977\n",
      "[269]\teval-error:0.093062\ttrain-error:0.035609\n",
      "[270]\teval-error:0.094163\ttrain-error:0.035609\n",
      "[271]\teval-error:0.093612\ttrain-error:0.035977\n",
      "[272]\teval-error:0.093612\ttrain-error:0.034875\n",
      "[273]\teval-error:0.093612\ttrain-error:0.034875\n",
      "[274]\teval-error:0.093062\ttrain-error:0.034508\n",
      "[275]\teval-error:0.093062\ttrain-error:0.034875\n",
      "[276]\teval-error:0.093062\ttrain-error:0.035242\n",
      "[277]\teval-error:0.093612\ttrain-error:0.034875\n",
      "[278]\teval-error:0.094163\ttrain-error:0.034508\n",
      "[279]\teval-error:0.094163\ttrain-error:0.034508\n",
      "[280]\teval-error:0.094714\ttrain-error:0.034508\n",
      "[281]\teval-error:0.094163\ttrain-error:0.034508\n",
      "[282]\teval-error:0.093612\ttrain-error:0.034141\n",
      "[283]\teval-error:0.094714\ttrain-error:0.034508\n",
      "[284]\teval-error:0.096916\ttrain-error:0.034141\n",
      "[285]\teval-error:0.096366\ttrain-error:0.034141\n",
      "[286]\teval-error:0.096916\ttrain-error:0.034141\n",
      "[287]\teval-error:0.096916\ttrain-error:0.033774\n",
      "[288]\teval-error:0.096916\ttrain-error:0.033407\n",
      "[289]\teval-error:0.096916\ttrain-error:0.033407\n",
      "[290]\teval-error:0.096366\ttrain-error:0.032305\n",
      "[291]\teval-error:0.096366\ttrain-error:0.032305\n",
      "[292]\teval-error:0.095815\ttrain-error:0.032305\n",
      "[293]\teval-error:0.096366\ttrain-error:0.031571\n",
      "[294]\teval-error:0.097467\ttrain-error:0.031571\n",
      "[295]\teval-error:0.097467\ttrain-error:0.031571\n",
      "[296]\teval-error:0.098018\ttrain-error:0.031571\n",
      "[297]\teval-error:0.098018\ttrain-error:0.031571\n",
      "[298]\teval-error:0.098018\ttrain-error:0.031938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-00649fce9d1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mnum_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mevallist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevallist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We can watch outputs to see model slowly optimize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m#Breaks 90% accuracy at around 410 iterations, with vanishing returns around 500.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dspy3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dspy3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dspy3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.asarray(X), \n",
    "                                                    np.asarray(Y),\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "Y_train = np.where(y_train=='Austen', 1, 0)\n",
    "Y_test = np.where(y_test=='Austen', 1, 0)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=Y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 5, \n",
    "    'eta': .2,\n",
    "    'silent': 0, \n",
    "    'objective': \n",
    "    'binary:logistic', \n",
    "    'scale_pos_weight': (class_imb),  #Tell model about class imbalance\n",
    "    'nthread':4,\n",
    "    'eval_metric':'error'\n",
    "}\n",
    "\n",
    "plst = param.items()\n",
    "num_round = 500\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "bst = xgb.train(plst, dtrain, num_round, evallist, verbose_eval=True) #We can watch outputs to see model slowly optimize.\n",
    "#Breaks 90% accuracy at around 410 iterations, with vanishing returns around 500.\n",
    "\n",
    "Y_ = np.where(bst.predict(dtrain)>.5, 1, 0)\n",
    "tb = pd.crosstab(Y_, Y_train)\n",
    "print(tb)\n",
    "print((tb.iloc[0, 0]+tb.iloc[1, 1])/tb.sum().sum())\n",
    "\n",
    "Y_ = np.where(bst.predict(dtest)>.5, 1, 0)\n",
    "tb = pd.crosstab(Y_, Y_test)\n",
    "print(tb)\n",
    "print((tb.iloc[0, 0]+tb.iloc[1, 1])/tb.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tkn = word_counts.iloc[123].text_sentence[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 'be', 'begin', 'to', 'get', 'very', 'tired', 'of', 'sit', 'by', '-PRON-', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'have', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', '-PRON-', 'have', 'peep', 'into', 'the', 'book', '-PRON-', 'sister', 'be', 'read', ',', 'but', '-PRON-', 'have', 'no', 'picture', 'or', 'conversation', 'in', '-PRON-', ',', \"'\", 'and', 'what', 'be', 'the', 'use', 'of', 'a', 'book', ',', \"'\", 'think', 'alice', \"'\", 'without', 'picture', 'or', 'conversation', '?', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "vctr = CountVectorizer(\n",
    "    ngram_range = (1, 3),\n",
    "    stop_words = 'english',\n",
    "    min_df = 3\n",
    ")\n",
    "lem_sents = []\n",
    "for sent in word_counts.text_sentence:\n",
    "    lem_sents.append([token.lemma_ for token in sent])\n",
    "\n",
    "print(lem_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recombined = [' '.join(x) for x in lem_sents]\n",
    "word_counts['recombined_'] = [' '.join(x) for x in lem_sents]\n",
    "\n",
    "vec_mat_tot = vctr.fit_transform(word_counts.recombined_)\n",
    "vec_mat_aus = vctr.transform(word_counts[word_counts.text_source == 'Austen'].recombined_)\n",
    "vec_mat_car = vctr.transform(word_counts[word_counts.text_source == 'Carroll'].recombined_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_vec = np.asarray(vec_mat_tot.todense().sum(axis=0))[0]\n",
    "aus_vec = np.asarray(vec_mat_aus.todense().sum(axis=0))[0]\n",
    "car_vec = np.asarray(vec_mat_car.todense().sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aus_vec_part = aus_vec/tot_vec\n",
    "car_vec_part = car_vec/tot_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_parts = np.array([aus_vec_part, car_vec_part])\n",
    "min_parts = min_parts.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3008"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pd.Series(index=range(0, len(vctr.vocabulary_)))\n",
    "for item in vctr.vocabulary_.items():\n",
    "    vocab[item[1]] = item[0]\n",
    "\n",
    "cutoff = .5\n",
    "new_vocab = np.where(min_parts < cutoff, vocab, '')\n",
    "new_vocab = list(set(new_vocab))\n",
    "new_vocab.remove('')\n",
    "len(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "0.930616740088 {'alpha': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  40 out of  40 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "new_vec = CountVectorizer(vocabulary=new_vocab)\n",
    "new_vec.fit([])\n",
    "X = new_vec.transform(word_counts.recombined_)\n",
    "mod = BernoulliNB()\n",
    "params = {\n",
    "    'alpha': [.2, .6, .4, .5]\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(mod, params, cv=10, verbose=1, n_jobs=2)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   3.1s\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   3.1s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   2.0s\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   2.0s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   2.2s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   2.2s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   2.0s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   2.0s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   2.1s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   2.1s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   2.8s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   2.8s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   2.3s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   2.3s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   2.9s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   3.0s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   3.0s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   3.1s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   2.4s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   2.4s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.7s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.5s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.6s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.5s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.7s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.9s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.8s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.7s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.5s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   1.4s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.5s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.6s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.6s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.7s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.6s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.6s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.8s\n",
      "[CV] C=1, gamma=1 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................................... C=1, gamma=1, total=   2.8s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.6s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   2.5s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.1s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.2s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   1.1s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   2.4s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   2.5s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   2.6s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   2.6s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   3.2s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   3.3s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   2.6s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   2.5s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   2.4s\n",
      "[CV] .................................... C=10, gamma=1, total=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895154185022 {'C': 1, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(class_weight='balanced')\n",
    "params = {\n",
    "    'C': [.1, 1, 10],\n",
    "    'gamma': [.1, 1]\n",
    "}\n",
    "\n",
    "srch = GridSearchCV(svc, params, cv=10, verbose=2, n_jobs=2)\n",
    "srch.fit(X, Y)\n",
    "print(srch.best_score_, srch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Environment (conda_dspy3)",
   "language": "python",
   "name": "conda_dspy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
